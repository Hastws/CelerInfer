{
  "model_type": "minimind",
  "model_name": "MiniMind - Baseline Micro Implementation",
  "description": "A lightweight transformer-based LLM with support for attention, FFN, and optional MoE",
  "cpp_executable": "minimind",
  "weights_file": "minimind.json",
  "config": {
    "hidden_size": 64,
    "num_hidden_layers": 2,
    "num_attention_heads": 8,
    "num_key_value_heads": 8,
    "intermediate_size": 256,
    "vocab_size": 128,
    "max_position_embeddings": 2048,
    "rope_theta": 10000.0,
    "rms_eps": 1e-6,
    "use_moe": false,
    "num_experts_per_tok": 2,
    "n_routed_experts": 8,
    "n_shared_experts": 2
  },
  "test_config": {
    "batch_size": 2,
    "seq_length": 5,
    "input_path": "data/input/test_input.bin",
    "output_path": "data/output/test_output.npy"
  }
}
