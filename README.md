# CelerInfer - Modular Multi-Model LLM Inference Framework

[![License](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)
![Python](https://img.shields.io/badge/python-3.12.11-blue)
![C++](https://img.shields.io/badge/C%2B%2B-14-blue)

CelerInfer is a hybrid **C++/Python LLM inference framework** designed for efficient model inference and debugging. It bridges PyTorch model training with optimized C++ tensor operations, supporting multiple model architectures (MiniMind, LLAMA, etc.).

## âœ¨ Key Features

- ğŸ”„ **Multi-Model Support** - Modular architecture for easy model integration
- ğŸš€ **Unified CLI** - Single command interface for all operations (`python -m python <cmd>`)
- ğŸ“Š **PyTorch â†” C++ Verification** - Automatic consistency checking
- ğŸ” **Layer-wise Debugging** - Extract and compare intermediate outputs
- ğŸ“š **Complete Documentation** - Architecture guide and extension examples
- âš™ï¸ **Automated Scripts** - Build, validate, and cleanup with convenience scripts

## ğŸš€ Quick Start

### 1. Environment Setup

```bash
# Create conda environment
conda create -n CelerInfer python=3.12.11
conda activate CelerInfer

# Install dependencies (if needed)
pip install torch transformers numpy
```

### 2. Using the Unified CLI

```bash
# List available models
python -m python list-models

# Export model weights to JSON
python -m python dump --model minimind

# Verify PyTorch â†” C++ consistency
python -m python validate --model minimind

# Run debugging tools
python -m python debug --model minimind
python -m python debug --model minimind --layer 0
```

### 3. Using Convenience Scripts

```bash
# Build C++ inference engine
bash scripts/build_cpp.sh

# Run complete validation pipeline
bash scripts/run_validation.sh minimind

# Clean build artifacts
bash scripts/clean.sh
```

## ğŸ“ Project Structure

```
CelerInfer/
â”œâ”€â”€ python/                    # Core Python module
â”‚   â”œâ”€â”€ __init__.py           # Package entry point
â”‚   â”œâ”€â”€ __main__.py           # CLI entry point
â”‚   â”œâ”€â”€ core/                 # Model definitions
â”‚   â”œâ”€â”€ export/               # Weight dumping (PyTorch â†’ JSON)
â”‚   â”œâ”€â”€ inference/            # Inference & verification
â”‚   â”œâ”€â”€ debug/                # Debugging & layer extraction
â”‚   â”œâ”€â”€ validate/             # Validation & comparison
â”‚   â”œâ”€â”€ utils/                # Common utilities
â”‚   â””â”€â”€ tools/                # Additional tools
â”‚
â”œâ”€â”€ cpp/                       # C++ inference engine
â”‚   â”œâ”€â”€ CMakeLists.txt
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ models/           # Model implementations
â”‚   â”‚   â”œâ”€â”€ ops/              # Tensor operations
â”‚   â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â””â”€â”€ inference/
â”‚   â”œâ”€â”€ include/              # Public headers
â”‚   â”œâ”€â”€ third_party/          # Dependencies (nlohmann JSON)
â”‚   â””â”€â”€ build/                # Build output
â”‚
â”œâ”€â”€ models/                    # Model configs & weights
â”‚   â”œâ”€â”€ minimind/             # MiniMind model
â”‚   â”‚   â”œâ”€â”€ config.json       # Configuration
â”‚   â”‚   â””â”€â”€ minimind.json     # Weights
â”‚   â””â”€â”€ llama/                # LLAMA (placeholder)
â”‚
â”œâ”€â”€ scripts/                   # Convenience shell scripts
â”‚   â”œâ”€â”€ build_cpp.sh
â”‚   â”œâ”€â”€ run_validation.sh
â”‚   â””â”€â”€ clean.sh
â”‚
â”œâ”€â”€ docs/                      # Project documentation
â”‚   â”œâ”€â”€ ARCHITECTURE.md       # Architecture & extension guide
â”‚   â”œâ”€â”€ MODELS.md             # Supported models
â”‚   â”œâ”€â”€ archives/             # Historical documents
â”‚   â””â”€â”€ legacy/               # Old debug scripts
â”‚
â”œâ”€â”€ data/                      # Test data
â”‚   â”œâ”€â”€ input/
â”‚   â””â”€â”€ output/
â”‚
â””â”€â”€ README.md (this file)
```

## ğŸ“– Documentation

- **[docs/ARCHITECTURE.md](docs/ARCHITECTURE.md)** - Detailed architecture, design patterns, and guide for adding new models
- **[docs/MODELS.md](docs/MODELS.md)** - List of supported models and their configurations
- **[REFACTORING_SUMMARY.md](REFACTORING_SUMMARY.md)** - Summary of recent project reorganization
- **.github/copilot-instructions.md** - AI agent instructions for project context

## ğŸ¯ Supported Models

### MiniMind âœ…
- **Type**: Transformer with RoPE, RMSNorm, SiLU
- **Config**: Hidden=64, Layers=2, Heads=8, FFN=256
- **Features**: Attention, FFN, optional MoE support
- **Status**: Fully implemented and verified

### LLAMA ğŸ“‹
- **Status**: Planned
- **Expected**: Support for LLAMA 2/3 models

### Qwen ğŸ“‹
- **Status**: Planned

## ğŸ”§ Core Workflows

### Workflow 1: Train â†’ Dump â†’ Verify â†’ Infer

```
PyTorch Model
    â†“
python/core/minimind_model.py (Define architecture)
    â†“
python/export/minimind_dumper.py (Export to JSON)
    â†“
python/inference/minimind_forward.py (Verify against PyTorch)
    â†“
cpp/src/models/minimind.cpp (C++ Inference)
    â†“
python/validate/compare_*.py (Compare outputs)
```

### Workflow 2: Quick Validation

```bash
# One-line validation
bash scripts/run_validation.sh minimind
```

## ğŸ› ï¸ Adding a New Model

See [docs/ARCHITECTURE.md#adding-a-new-model](docs/ARCHITECTURE.md#adding-a-new-model) for complete guide.

Quick summary:
1. Create `models/mymodel/config.json`
2. Implement `python/core/mymodel_model.py`
3. Implement `python/export/mymodel_dumper.py`
4. Implement `python/inference/mymodel_forward.py`
5. Register in `python/core/__init__.py`
6. Implement C++ version in `cpp/src/models/mymodel.cpp`

## ğŸ“Š Project Statistics

| Aspect | Count |
|--------|-------|
| Python Modules | 7 (core, export, inference, debug, validate, utils, tools) |
| Debug Scripts | 5+ |
| Comparison Scripts | 10+ |
| C++ Files | Modular (src/, ops/, utils/) |
| Documentation Files | 4+ |

## ğŸ—ï¸ Architecture Highlights

### Python Module Organization
- **core**: Model definitions with factory pattern
- **export**: Unified weight dumping interface
- **inference**: Verification against PyTorch
- **debug**: Layer extraction and analysis
- **validate**: Comprehensive comparison tools

### C++ Design
- Header-only tensor operations for efficiency
- Factory pattern for model instantiation
- Struct-based inference (no class overhead)
- JSON+Base64 weight format for transparency

### CLI Interface
- Single entry point: `python -m python <command>`
- Subcommands: dump, validate, debug, list-models
- Model-agnostic design

## âš™ï¸ Build & Test

### Building C++ Engine
```bash
bash scripts/build_cpp.sh
# Or manually:
cd cpp && mkdir -p build && cd build && cmake .. && make
```

### Running Validation
```bash
# Complete validation pipeline
bash scripts/run_validation.sh minimind

# Or step by step:
python -m python dump --model minimind
python -m python validate --model minimind
```

### Cleaning Artifacts
```bash
bash scripts/clean.sh
```

## ğŸ“ Environment

- **Python**: 3.12.11
- **C++ Standard**: C++14
- **Dependencies**: 
  - Python: torch, transformers, numpy
  - C++: nlohmann/json (included)
- **Build System**: CMake 3.16+

## ğŸ¤ Contributing

To add a new model or feature:
1. Read [docs/ARCHITECTURE.md](docs/ARCHITECTURE.md)
2. Follow the module structure
3. Add documentation
4. Test thoroughly
5. Commit with clear messages

## ğŸ“œ License

MIT License - See [LICENSE](LICENSE) for details

## ğŸ”— References

- **Project Refactoring**: See [REFACTORING_SUMMARY.md](REFACTORING_SUMMARY.md)
- **Architecture Details**: See [docs/ARCHITECTURE.md](docs/ARCHITECTURE.md)
- **Model List**: See [docs/MODELS.md](docs/MODELS.md)
- **Legacy Docs**: See [docs/archives/](docs/archives/)

## ğŸ“ Quick Links

| Resource | Location |
|----------|----------|
| CLI Entry | [python/__main__.py](python/__main__.py) |
| Model Registry | [python/core/__init__.py](python/core/__init__.py) |
| MiniMind Config | [models/minimind/config.json](models/minimind/config.json) |
| Architecture Guide | [docs/ARCHITECTURE.md](docs/ARCHITECTURE.md) |
| Validation Scripts | [scripts/](scripts/) |

---

**Last Updated**: January 27, 2026  
**Version**: 0.1.0  
**Status**: âœ… Multi-model architecture ready
